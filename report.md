#Report


## Actor Network


The actor network had three fully-conncetd layers. We used Batch Normalization in all layers except the output layer. The Relu activation function was used in all neurons except output layer which had tanh activation. The number of neurons in hidden layers were 100, 100, and 4 (action size).
