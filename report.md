# Report


## Actor Network


The actor network had three fully-conncetd layers. We used Batch Normalization in all layers except the output layer. The Relu activation function was used in all neurons except output layer which had tanh activation. The number of neurons in the hidden layers were 100, 100, and 4 (action size).


## Critic Network


The critic network had three fully-conncetd layers with respective 100, 100, and 1 neurons. The Relu activation function was used in all neurons except output layer which did not have activation function.
